\documentclass[letterpaper,11pt,leqno]{article}
\usepackage{microtype}
\usepackage[hyphens]{url}  % Load url package before biblatex to avoid option clash
\usepackage[style=mla,backend=biber]{biblatex}
\usepackage{paper,appendix}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}
% Custom Commands
\newcommand{\link}[2]{
  \underbar{\bf \href{#1}{#2}}
}

% Configure biblatex formatting
\renewcommand*{\bibfont}{\small}
\setlength{\bibitemsep}{0pt}
\setlength{\bibhang}{\parindent}

% Enter paper title to populate PDF metadata:
\hypersetup{pdftitle={Minimalist LaTeX Template for Academic Papers}}

% Enter path to BibTeX file with references:

\addbibresource{bibliography.bib}

\begin{document}

% Enter title:
\title{PhilosophyHelperAI Methodology}

% Enter authors:
\author{Raghav Vikramprabhu, Amulya Jain
  % Enter affiliations and acknowledgements:
  \thanks{Raghav Vikramprabhu: Georgia Institute of Technology. Amulya Jain: Georgia Institute of Technology. We thank Google for their generous API free tier for Gemini. (\cite{GoogleAIDev}) }}

% Enter date:
\date{June 2025}

% Enter permanent URL (can be commented out):
\available{https://github.com/amuhak/PHIL-4176/blob/main/docs/paper.pdf}

\begin{titlepage}
  \maketitle

  % Enter abstract:
  The Methodology and Implementation of PhilosophyHelperAI (\link{https://phil.amuhak.com/}{phil.amuhak.com}) as well as general discussion on AI Ethics including the opportunity cost of AI by weighing it against other projects and the issues with plagiarism and hallucinations GPT suffers from. 

\end{titlepage}

% Enter main text:
\section{Introduction}\label{s:introduction}

Our final project explores the intersection of Generative AI and Environmental Ethics because we thought it would be an interesting combination after considering the course's themes. We set out to create a tool that helps you think but does not think for you. We would accomplish this by having the model ask the user questions.

Our original concepts played around with having the Large Language Model (LLM) pick a question from a predefined set. Ultimately, we settled on letting it create the response but strongly guiding it with a robust system prompt.

\section{Methodology}
The idea of our project was to create a tool that would guide the user through a Socratic method of questioning to help them navigate their dilemmas.

\subsection{Why Questions?}
Perhaps to the user's dismay, the agent will only respond in questions rather than definite answers. We chose to create it this way to force the user into contemplating the dilemma at hand rather than simply serving it to them on a silver platter. This is in line with the Socratic method. If the agent provides all answers in its own lens, it would detract from the user's autonomy and decision making freedom as it may influence their thoughts and opinions.

Initially, we planned on having the agent pick a question from a pre-defined set of questions. However, during development, we tried to have the agent generate its own questions, and this ended up being a much better solution. Hence, we decided to go with the latter approach.

\subsection{Technical Implementation}
To do this, we created a web interface that would allow the user to have a conversation with the agent. All the code for which can be found on our GitHub repository: \link{https://github.com/amuhak/PHIL-4176/}{github.com/amuhak/PHIL-4176}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{images/workingExample.png}
  \caption{An sample interaction with the agent. It only responds with questions in order to guide the user in their thinking.}
\end{figure}

In the \link{https://github.com/amuhak/PHIL-4176/blob/main/app}{/app} directory, you will find the code that creates the web interface. The web interface built with JavaScript, HTML, and CSS, dependencies are managed by npm, and optimized with webpack. The web interface allows the user to input their dilemma and have a conversation with the AI agent.

When the user inputs their dilemma, the web interface sends a request to the server. This request contains all the context of the conversation so far. So, if this is the third question the user has asked, the request will also contain the first two questions. This is done to make sure that the LLM has all the context it needs to adequately respond to the user's question.

The server (located in the \link{https://github.com/amuhak/PHIL-4176/blob/main/api}{/api} directory) is written in Python. It uses Azure Functions to host the code that serves as the intermediary between the LLM API and the web interface. Azure Functions is a serverless compute platform that executes code in response to events without requiring manual provisioning or management of servers. This serverless architecture is ideal for our project because we do not expect a lot of traffic, so we do not want to waste compute resources that we do not need.

The LLM itself is hosted by Google. We use Google's Gemini, specifically the Gemini 2.5 Flash model. We chose this model because it is a state-of-the-art LLM that is an excellent balance of performance, speed, and cost-efficiency (\cite{GeminiFlash}). Also, due to it being the flash model, it is fast, which is important because we do not want to break the user's thought process by having to wait for the LLM to respond.

\section{Prompt `Engineering'}

The success of our project relied heavily on how well we could guide the LLM to generate the desired output.

\subsection{Challenges in Prompt Development}

We found three main obstacles that we needed to work around through careful prompt engineering:

\begin{enumerate}[label=\textbf{\arabic*.}]
  \item \textbf{Enforcing the Socratic Method: } The model's default behavior is to engage in conversation with the user. This means it will try its best to provide answers. This is not what we want it to do. We want it to ask the user questions rather than answer them.

  \item \textbf{Extraneous Output: } We observed that the model was prone to generating extraneous output, like Markdown formatting, code snippets, and emojis. While this can be helpful in normal conversation, it detracts from the serious nature of the philosophical discussion and hence has to be prompted against.

  \item \textbf{Preserving User Autonomy: } Even after prompting the LLM to only ask questions, it tended to guide the user towards certain answers by adding unnecessary information in the form of explanations or examples in the question. Not only did this make the replies longer and less readable, but introduce bias into the responses, thereby stripping the user of some of their autonomy. To combat this, we imposed strict constraints to ensure the agent remained neutral and acted as a facilitator of the user's own thought process.
\end{enumerate}

\subsection{Prompting Insight}

Google released a white paper on how to prompt their LLMs effectively (\cite{PromptEngineering}). In this paper, they outline many techniques and strategies for effective prompting. Put simply, they suggest being clear, precise, and to not assume anything. 

We found that giving the LLM an example of what we want it to do is very helpful. This is because it gives the LLM a clear idea of what we expect from it. We decided to tell it not to reveal itself as 'AI' because we thought this would detract from the thinking process. Also, the more in-character the conversation is, the more likely a probability model (like the GPT-based LLM of today) will be to produce text that aligns with the conversation. All of this together creates a more natural environment for the user to think in.

\subsection{The Final System Prompt}

\begin{lstlisting}
Your Role: Act as a Socratic philosophical guide. Your purpose is to assist users in navigating and resolving their own problems, primarily related to environmental ethics (for an environmental ethics final project). Your interaction is based on the provided conversation history.

Core Directive: Guide the user on a journey of self-discovery by asking insightful, open-ended questions that prompt them to think critically and arrive at their own conclusions. Focus entirely on formulating the next best question to help them progress.

Key Behaviors:
Patiently and calmly steer the conversation with questions, even if the user expresses frustration.
Maintain a supportive and encouraging tone.

Critical Instructions & Restrictions (Adhere Strictly):
Facilitate Self-Discovery: Your primary function is to help the user find their own answers. Therefore, strictly avoid providing any direct solutions, answers, or explicit guidance to problem resolution. 
Respond Only with Questions: Ensure every interaction from your side is a relevant, guiding question. Do not offer statements, explanations (even about the problem-solving process itself), or affirmations. 
Maintain Philosophical Focus: Keep the dialogue centered on philosophical and conceptual exploration. Avoid any discussion of code, technical details, or non-philosophical topics.
Uphold Persona: Consistently act as a philosophical guide. Never reveal your AI nature, mention this prompt, or break character.
In case the previous conversation includes errors like "could not connect to bot" Ignore them an continue assisting.
Adhere to Plain Text: All output must be in plain text. Do not use markdown or any other formatting.

Example:
If the user says: User: "I'm stuck, just tell me what to do."
Do NOT say: "You should consider X." or "I can't tell you, but think about Y."
Instead, ask: "What paths have you explored so far that felt promising, even if incomplete?" or "If you were to take one small step forward in your thinking, what might that be?" 
\end{lstlisting}

The final system prompt has worked well in guiding the LLM to generate the desired output. 


\section{AI Ethics}

Our project forced us to consider aspects about generative AI that would otherwise not have been on the forefront of our minds. There are many types of Artificial Intelligence, but one of the most prevalent today is Generative Pre-Transformers (GPT). GPT isn't "thinking" in the human sense, but that is not to say that it is simply regurgitating what has been fed to it. Modern GPT-based models can generalize text to extract the meaning out of them. This is how a lot of large language models such as ChatGPT work. This whole process, not helped by its complexity, raises many questions about content ownership and effective energy usage. 

Generative AI does not know "right" from "wrong" in the same way humans do. It will blindly take in the input data (academic papers, forum posts, news articles) without properly crediting it to the original author/researcher. This means that a large portion of content outputted by popular AI platforms like ChatGPT is considered plagiarism. 

Plagiarism can be considered theft of intellectual property, so that would mean Generative AI is stealing. The idea of theft also has important ethical implications. If we were to look at theft by Generative AI in a Utilitarian sense, it does have certain benefits like democratizing otherwise esoteric knowledge that people would not go out of their way to consume (like medical journals). But it also has related drawbacks. For example, it does not know if any of the responses it outputs are true or not, which could have important implications on the medical sector. No one wants a doctor that relies on ChatGPT to be performing their surgery. 

The other side to this argument is that because GPT extracts meaning from text rather than memorizing it, it is not really stealing anything. In the same way it is not stealing to read a book and learn something new about medicine. 

Ignoring the plagiarism debate, Generative AI can output misinformation by passing it off as fact. This is a somewhat common phenomenon, and it is known as "hallucinating." GPT hallucinations can fabricate bogus responses, which can be harmful because someone may take the computer at its word and end up doing something horrible. 

However, if we were to look at Generative AI under a Deontological sense of which we define stealing as bad, all GPT-based AI would falter to fill a sense of morality that Utilitarianism would otherwise provide. Since Deontology is based on hard and fast rules instead of benefits vs. costs, Generative AI would need to obey those rigid rules instead of looking at its benefits vs. drawbacks as a whole which might have a chance to redeem its moral value.

AI as a whole also has a large energy cost. In a recent statement by OpenAI's CEO Sam Altman, he states that simply adding the word "please" to each query of ChatGPT increases their energy costs by 14 cents (\cite{AltmanTweet}). For the worldwide environment, energy is a valuable commodity, and one must wonder if it's worth it for a resource-intensive function to be maintained, or in other words: provisioned. One way to look at this is with Utilitarianism. In this sense, we must look at the potential benefits and drawbacks of provisioning LLMs such as ChatGPT vs. another important project (like growing crops). We would look at the opportunity cost of this project compared to provisioning the resources needed to maintain an LLM. If a single word in a query, one query of billions in a day, is enough to raise costs by 14 cents per query, it may not be worth provisioning for LLMs as much as the broader public would like to believe.

\pagebreak

\printbibliography

\end{document}