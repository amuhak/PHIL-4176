\documentclass[letterpaper,11pt,leqno]{article}
\usepackage{microtype}
\usepackage[hyphens]{url}  % Load url package before biblatex to avoid option clash
\usepackage[style=mla,backend=biber]{biblatex}
\usepackage{paper,appendix}
\usepackage{graphicx}
\usepackage{indentfirst}

% Custom Commands
\newcommand{\link}[2]{
  \underbar{\bf \href{#1}{#2}}
}

% Configure biblatex formatting
\renewcommand*{\bibfont}{\small}
\setlength{\bibitemsep}{0pt}
\setlength{\bibhang}{\parindent}

% Enter paper title to populate PDF metadata:
\hypersetup{pdftitle={Minimalist LaTeX Template for Academic Papers}}

% Enter path to BibTeX file with references:

\addbibresource{bibliography.bib}

\begin{document}

% Enter title:
\title{PhilosophyHelperAI Methodology}

% Enter authors:
\author{Raghav Vikramprabhu, Amulya Jain
  % Enter affiliations and acknowledgements:
  \thanks{Raghav Vikramprabhu: Georgia Institute of Technology. Amulya Jain: Georgia Institute of Technology. We thank Google for releasing their paper on how to properly prompt an LLM. (\cite{PromptEngineering}) }}

% Enter date:
\date{June 2025}

% Enter permanent URL (can be commented out):
\available{https://github.com/amuhak/PHIL-4176/blob/main/docs/paper.pdf}

\begin{titlepage}
  \maketitle

  % Enter abstract:
  This is the abstract.

\end{titlepage}

% Enter main text:
\section{Introduction}\label{s:introduction}

Our final project explores the intersection of Generative AI and Environmental Ethics because we thought it would be an interesting combination after considering the course's themes. We set out to create a tool that helps you think but does not think for you. We would accomplish this by having the modal ask the user questions.

Our original concepts played with having the Large Language Model (LLM) pick a question from a predefined set. Ultimately, we settled on letting it create the response but strongly guiding it with a robust system prompt.

\section{Methodology}
The idea of our project was to create a tool that would guide the user through a Socratic method of questioning to help them navigate their dilemmas.

\subsection{Why Questions?}
The reason why we choose to have the agent only respond in questions rather than definite answers, to the users' dismay, is because we wanted to force the user to think about their dilemma rather than just giving them the answer. This is in line with the Socratic method of questioning. If the agent were to give the user a definite answer, it would be taking away the user's autonomy and decision making freedom.

In the beginning, we were planning on having the agent pick a question from a predefined set of questions. However, during development, we tried to have the agent generate its own questions, and this ended up being a much better solution. Hence, we decided to go with the latter approach.

\subsection{Technical Implementation}
To do this, we created a web interface that would allow the user to have a conversation with the agent. All the code for which can be found on our GitHub repository: \link{https://github.com/amuhak/PHIL-4176/}{github.com/amuhak/PHIL-4176}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{images/workingExample.png}
  \caption{An sample interaction with the agent. It only responds with questions in order to guide the user in their thinking.}
\end{figure}

In the \link{https://github.com/amuhak/PHIL-4176/blob/main/app}{/app} directory, you will find the code that creates the web interface. The web interface built with JavaScript, HTML, and CSS, dependencies are managed by npm, and optimized with webpack. The web interface allows the user to input their dilemma and have a conversation with the AI agent.

When the user inputs their dilemma, the web interface sends a request to the server. This request contains all the context of the conversation so far. So If this is the third question the user has asked, the request will also contain the first two questions. This is done to make sure that the LLM has all the context it needs to adequately respond to the user's question.

The server itself is written in Python (located in the \link{https://github.com/amuhak/PHIL-4176/blob/main/api}{/api} directory). It uses Azure Functions to host the code that serves as the intermediary between the LLM API and the web interface. Azure Functions is a serverless compute platform that executes code in response to events without requiring manual provisioning or management of servers. This serverless architecture is ideal for our project because we do not expect a lot of traffic, so we do not want to use compute resources that we do not need.

The LLM itself is hosted by Google. We use Google's Gemini, specifically the Gemini 2.5 Flash model. We chose this model because it is a state-of-the-art LLM that is an excellent balance of performance, speed, and cost-efficiency (\cite{GeminiFlash}). Also, due to it being the flash model, it is fast, which is important because we do not want to break the user's thought process by having to wait for the LLM to respond.

\section{Challenges}

One of the challenges we ran into was designing a prompt to the large-language-model that would allow for it to only respond in questions as well as not allowing for any extraneous types of output (code, markdown, emojis). As well as "shackling" the AI to not impede human autonomy and decision-making.

Our final prompt can be seen here:

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{images/prompt.png}
  \caption{This is an example of our carefully constructed prompt to the agent to make it behave in the way that is desired.}
\end{figure}

\section{AI Ethics}

Our project forced us to consider aspects about generative AI that otherwise would not have been on the forefront of our minds. Generative Pre-Transformer AI isn't actually "thinking" in the human sense but rather "rephrasing" whatever data has been fed into it. This is how a lot of large language models such as ChatGPT work (GPT stands for Generative Pre-Transformer). This whole process raises a lot of questions about content ownership as well as takes a large amount of energy to sustain.

Generative AI largely does not know "right" from "wrong" in a traditional sense so it will blindly take in the input data (academic papers, internet posts, without properly crediting it to the original author/researcher. This means that a large portion of content outputted by popular AI platforms like ChatGPT is considered plagiarism.

Plagiarism can be considered theft of intellectual property in a sense so that would mean Generative AI is stealing. The idea of theft has important ethical implications as well. If we were to look at theft by Generative AI in a Utilitarian sense, it does have certain benefits like democratizing otherwise esoteric knowledge that people would not go out of their way to consume (like medical journals). But it also has related drawbacks. One such of these being unknown if any of the responses it outputs is true or not which could have important implications on the medical, legal, and political sectors to name a few.

However, if we were to look at Generative AI under a Deontological sense of which we define stealing as bad, all Generative Pre Transformer based AI would falter to fill a sense of morality that Utilitairianism would otherwise provide. Since Deontology is based on hard and fast rules instead of benefits vs. costs, Generative AI would need to obey those rules instead of looking at its benefits vs. drawbacks which might have a chance to redeem its moral value.

AI as a whole also has a large energy cost. In a recent statement by OpenAI's CEO Sam Altman, he states that simply adding the word "please" to each query of ChatGPT increases their energy costs by 14 cents. For the worldwide environment, energy is a valuable commodity and one must wonder if its worth it for a resource-intensive function to be maintained, or in other words: provisioned. One way to look at this is with Utilitarianism. In this sense, we must look at the potential benefits and drawbacks of provisioning a LLM such as ChatGPT vs. growing crops. We would look at the oppurtunity cost of a certain project (like growing crops) as opposed to provisioning the resources needed to maintain an LLM. If a single word in a query, one of billions, is enough to raise costs by 14 cents, it may not be worth provisioning for LLMs as much as the broader public would like to believe.

Of course, new technological advancements in neural networks could end up replicating the human mind to a point where it would be in humans' best interest to provision for AI because it could help us solve problems on a superhuman scale because a machine doesn't need any of the physiological needs like a human.

We would like to think that this Artificial General Intelligence wouldn't replace humans completely but being able to infinitely scale human intelligence to a virtual server rack with hundreds of instances might make humans obsolete.

However that type of technology will take a long time, to say the least. Humanity might not even get to that point because of the aforementioned energy limitations so its important to stay focused on the present usage and ethics of AI.

\pagebreak

\printbibliography

\end{document}